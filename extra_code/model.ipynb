{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d230245-c100-4dfe-ae0b-5c4337117c44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk) (4.66.4)\n",
      "Using cached nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "Using cached regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (800 kB)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.9.2 regex-2025.11.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting language-tool-python\n",
      "  Downloading language_tool_python-3.0.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from language-tool-python) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from language-tool-python) (4.66.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from language-tool-python) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from language-tool-python) (5.9.8)\n",
      "Collecting toml (from language-tool-python)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->language-tool-python) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->language-tool-python) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->language-tool-python) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->language-tool-python) (2025.1.31)\n",
      "Downloading language_tool_python-3.0.0-py3-none-any.whl (48 kB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Installing collected packages: toml, language-tool-python\n",
      "Successfully installed language-tool-python-3.0.0 toml-0.10.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting pyspellchecker\n",
      "  Using cached pyspellchecker-0.8.3-py3-none-any.whl.metadata (9.5 kB)\n",
      "Using cached pyspellchecker-0.8.3-py3-none-any.whl (7.2 MB)\n",
      "Installing collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.8.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting install-jdk\n",
      "  Using cached install_jdk-1.1.0-py3-none-any.whl.metadata (12 kB)\n",
      "Using cached install_jdk-1.1.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: install-jdk\n",
      "Successfully installed install-jdk-1.1.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No java install detected. Please install java to use language-tool-python.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# grammar error\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlanguage_tool_python\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m tool \u001b[38;5;241m=\u001b[39m \u001b[43mlanguage_tool_python\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLanguageTool\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men-US\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# spell checker\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspellchecker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpellChecker\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/language_tool_python/server.py:179\u001b[0m, in \u001b[0;36mLanguageTool.__init__\u001b[0;34m(self, language, mother_tongue, remote_server, new_spellings, new_spellings_persist, host, config, language_tool_download_version)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server_is_alive():\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_port\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/v2/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_start_server_on_free_port\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m language \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/language_tool_python/server.py:601\u001b[0m, in \u001b[0;36mLanguageTool._start_server_on_free_port\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 601\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_start_local_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ServerError:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/language_tool_python/server.py:623\u001b[0m, in \u001b[0;36mLanguageTool._start_local_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;124;03mStart the local LanguageTool server.\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;124;03mThis method starts a local instance of the LanguageTool server. If the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;124;03m:raises ServerError: If the server fails to start or exits early.\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;66;03m# Before starting local server, download language tool if needed.\u001b[39;00m\n\u001b[0;32m--> 623\u001b[0m \u001b[43mdownload_lt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage_tool_download_version\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_port:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/language_tool_python/download_lt.py:253\u001b[0m, in \u001b[0;36mdownload_lt\u001b[0;34m(language_tool_version)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_lt\u001b[39m(language_tool_version: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m LTP_DOWNLOAD_VERSION) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    240\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;124;03m    Downloads and extracts the specified version of LanguageTool.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m    This function checks for Java compatibility, and downloads the specified version of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m    :raises ValueError: If the specified version format is invalid.\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m     \u001b[43mconfirm_java_compatibility\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage_tool_version\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m     download_folder \u001b[38;5;241m=\u001b[39m get_language_tool_download_path()\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;66;03m# Use the env var to the jar directory if it is defined\u001b[39;00m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# otherwise look in the download directory\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/language_tool_python/download_lt.py:101\u001b[0m, in \u001b[0;36mconfirm_java_compatibility\u001b[0;34m(language_tool_version)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m java_path:\n\u001b[1;32m     98\u001b[0m     err \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo java install detected. Please install java to use language-tool-python.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m     )\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(err)\n\u001b[1;32m    103\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound java executable at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, java_path)\n\u001b[1;32m    105\u001b[0m output \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mcheck_output(  \u001b[38;5;66;03m# noqa: S603  # java_path come from shutil.which -> trusted\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     [java_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-version\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    107\u001b[0m     stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mSTDOUT,\n\u001b[1;32m    108\u001b[0m     universal_newlines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    109\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No java install detected. Please install java to use language-tool-python."
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install language-tool-python\n",
    "!pip install pyspellchecker\n",
    "!pip install install-jdk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import jdk\n",
    "import os\n",
    "# download java\n",
    "jdk.install('17')\n",
    "# !export JAVA_HOME=$HOME/.jdk/jdk-17.0.17+10\n",
    "# !export PATH=$PATH:$JAVA_HOME/bin\n",
    "# jdk_version = 'jdk-17.0.17+10' #change with your version \n",
    "#os.environ['JAVA_HOME'] = '/home/jovyan/.jdk/jdk-17.0.17+10'\n",
    "#os.environ['PATH'] = f\"{os.environ.get('PATH')}:{os.environ.get('JAVA_HOME')}/bin\"\n",
    "# for word_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "# for pos_tag\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "# grammar error\n",
    "import language_tool_python\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "# spell checker\n",
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker(language='en')\n",
    "# parallelism for faster compute\n",
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9f0254e7-9b99-4d78-b05f-e2e98d1d8997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e22ec90-4300-46d9-9343-d79d3db8d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTENT_TAGS = {\n",
    "    # Nouns\n",
    "    \"NN\", \"NNS\", \"NNP\", \"NNPS\",\n",
    "    # Verbs\n",
    "    \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\",\n",
    "    # Adjectives\n",
    "    \"JJ\", \"JJR\", \"JJS\",\n",
    "    # Adverbs\n",
    "    \"RB\", \"RBR\", \"RBS\",\n",
    "}\n",
    "\n",
    "DERIVATIONAL_SUFFIXES = {\n",
    "    \"tion\", \"sion\", \"ment\", \"ness\", \"ity\", \"ance\", \"ism\", \"ship\", \"hood\",\n",
    "    \"ive\", \"ous\", \"able\", \"al\", \"ical\", \"ize\", \"ise\", \"ify\", \"ate\",\n",
    "    \"ary\", \"ory\", \"ant\", \"ent\", \"ery\", \"ist\"\n",
    "}\n",
    "\n",
    "def compute(text: str, total_words: int) -> np.ndarray:\n",
    "    text = str(text)\n",
    "    total_words = int(total_words)\n",
    "\n",
    "    # sentences and sentence lengths\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    sent_lengths = [\n",
    "        len(nltk.word_tokenize(sent))\n",
    "        for sent in sentences\n",
    "    ]\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Feature 1\n",
    "    num_sentences = len(sentences)\n",
    "\n",
    "    # Feature 2\n",
    "    avg_sent_len = total_words / num_sentences\n",
    "\n",
    "    # Feature 3\n",
    "    std_sent_len = float(np.std(sent_lengths))\n",
    "\n",
    "    # Feature 4\n",
    "    cleaned_tokens = [\n",
    "        w.replace('.', '').replace(',', '')\n",
    "        for w in tokens\n",
    "    ]\n",
    "    total_chars = sum(len(w) for w in cleaned_tokens)\n",
    "    chars_per_word = total_chars / total_words\n",
    "\n",
    "    # Feature 5\n",
    "    content_words = sum(1 for _, tag in tagged if tag in CONTENT_TAGS)\n",
    "    content_ratio_val = content_words / total_words\n",
    "\n",
    "    # Feature 6\n",
    "    total_suffixes = 0\n",
    "    for w in tokens:\n",
    "        wl = w.lower()\n",
    "        if len(wl) > 5 and any(wl.endswith(suffix) for suffix in DERIVATIONAL_SUFFIXES):\n",
    "            total_suffixes += 1\n",
    "    deriv_ratio_val = total_suffixes / total_words\n",
    "\n",
    "    # Feature 7\n",
    "    matches = tool.check(text)\n",
    "    error_ratio_val = (len(matches) / total_words)\n",
    "\n",
    "    # Feature 8\n",
    "    misspelled = spell.unknown(tokens)\n",
    "    spelling_error_ratio = len(misspelled) / total_words\n",
    "    \n",
    "    return np.array([\n",
    "        num_sentences,\n",
    "        avg_sent_len,\n",
    "        std_sent_len,\n",
    "        chars_per_word,\n",
    "        content_ratio_val,\n",
    "        deriv_ratio_val,\n",
    "        error_ratio_val,\n",
    "    ], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c6ec00-b9e8-4fdc-81d2-767a00843afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"english_exam_database.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "169c1fa8-f3ed-4082-ab06-73ee790553c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_row(args):\n",
    "    text, wc = args\n",
    "    return compute(text, wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9e3ff0a-1a40-4a0b-872b-d48ce26700cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cpu_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m wordcounts \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwordcount\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m      4\u001b[0m pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(texts, wordcounts))\n\u001b[0;32m----> 5\u001b[0m cpu_num \u001b[38;5;241m=\u001b[39m \u001b[43mcpu_count\u001b[49m()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(processes\u001b[38;5;241m=\u001b[39mcpu_num) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[1;32m      8\u001b[0m     feature_rows \u001b[38;5;241m=\u001b[39m pool\u001b[38;5;241m.\u001b[39mmap(compute_row, pairs, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cpu_count' is not defined"
     ]
    }
   ],
   "source": [
    "texts = df[\"text_corrected\"].to_numpy()\n",
    "wordcounts = df[\"wordcount\"].to_numpy()\n",
    "\n",
    "pairs = list(zip(texts, wordcounts))\n",
    "cpu_num = cpu_count()\n",
    "\n",
    "with Pool(processes=cpu_num) as pool:\n",
    "    feature_rows = pool.map(compute_row, pairs, chunksize=100)\n",
    "\n",
    "text_features = np.vstack(feature_rows)\n",
    "print(text_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89507d90-e8e9-4cea-9e0c-d2cb0a6320a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(317220, 2)\n"
     ]
    }
   ],
   "source": [
    "features = df[[\"wordcount\", \"mtld\"]].to_numpy(dtype=np.float32)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a19aeb2c-1c0f-40e4-a367-37d40d9906dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(317220, 11)\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate([features, X], axis=1)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ffea1525-04e5-402f-a381-aa71fbc611b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mX\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Should be N by 10\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f14359-f20e-4cd3-a926-b134c627d7be",
   "metadata": {},
   "source": [
    "**Training the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ef39c3e-01e5-4b13-8290-cb824aada158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149492\n",
      "85753\n",
      "55033\n",
      "85753\n",
      "4891\n"
     ]
    }
   ],
   "source": [
    "print(df[df['cefr'] == 'A1'].shape[0])\n",
    "print(df[df['cefr'] == 'A2'].shape[0])\n",
    "print(df[df['cefr'] == 'B1'].shape[0])\n",
    "print(df[df['cefr'] == 'A2'].shape[0])\n",
    "print(df[df['cefr'] == 'C1'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fa6e498-d24b-444f-9492-adec5bdb6094",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05e6b59d-86a0-48e7-bb00-63ae18e83f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [5.],\n",
       "       [5.],\n",
       "       [5.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df[[\"cefr_numeric\"]].to_numpy(dtype=np.float32)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59991d99-29d0-4118-b5dc-3ed19ca5b8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [5.],\n",
       "       [5.],\n",
       "       [5.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dafad1aa-619b-48c9-80d1-c5fb4b348d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_train_test_split(X: np.ndarray, y: np.ndarray):\n",
    "    train_ratio = 0.8\n",
    "    # turn y into 1 x N s.t. N is the number of data points\n",
    "    y = y.ravel()\n",
    "    train_ratio = 0.8\n",
    "\n",
    "    unique_classes = np.unqiue(y) # 1,2,3,4,5\n",
    "\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "\n",
    "    for c in unique_classes:\n",
    "        class_indices = np.where(y == c)[0]\n",
    "        np.random.shuffle(class_indices)\n",
    "        n_class = len(class_indices)\n",
    "        n_train_c = int(train_ratio * n_class)\n",
    "\n",
    "        train_indices.append(class_indices[:n_train_c])\n",
    "        test_indices.append(class_indices[n_train_c:])\n",
    "\n",
    "    # concatenate indices from all classes\n",
    "    train_indices = np.concatenate(train_indices)\n",
    "    test_indices = np.concatenate(test_indices)\n",
    "\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.radom.shuffle(test_indices)\n",
    "\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8984611-a148-4196-8055-49e9a46950c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE TO SELF: KEEP THESE TWO VARIABLES TO PREDICT NEW DATA LATER\n",
    "feature_means = X_train.mean(axis=0) # shape (10,)\n",
    "feature_stds = X_train.std(axis=0) # shape (10,)\n",
    "\n",
    "# edge case to avoid division by zero\n",
    "feature_std[feature_stds == 0] = 1.0\n",
    "\n",
    "# standardize train and test using the training info\n",
    "X_train_std = (X_train - feature_means) / feature_stds\n",
    "X_test_std = (X_test - feature_means) / feature_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc89fc04-f981-455c-b70c-59f051577397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change index for softmax to be easier\n",
    "y_train_classifier = y_train.astype(int) - 1\n",
    "y_test_classifier = y_test.astype(int) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b17b0e4-5c4b-4fb2-b8f8-f106d2c45f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
