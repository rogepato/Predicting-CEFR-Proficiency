{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94f7701-f6f8-4a63-94a3-6486726b7acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "class SoftmaxRegression:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int = 10,\n",
    "        n_classes: int = 5,\n",
    "        learning_rate: float = 0.1,\n",
    "        num_epochs: int = 100,\n",
    "        reg_lambda: float = 0.0\n",
    "    ):\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.reg_lambda = reg_lambda\n",
    "\n",
    "        # Model parameters\n",
    "        self.W: Optional[np.ndarray] = None\n",
    "        self.b: Optional[np.ndarray] = None\n",
    "\n",
    "        # track loss over epochs\n",
    "        self.loss_history = []\n",
    "\n",
    "    def _softmax(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        # check if copying is the most efficient/effective\n",
    "        Z = W^\\top x + b, Z \\in \\mathbb{R}^{N \\times 5}\n",
    "        Z = \\begin{bmatrix}\n",
    "                2.0, 1.0, 0.0, -1.0, -2.0\n",
    "                2.1, 3.4, 2.5, 0.2, -3.43\n",
    "            \\end{bmatrix}\n",
    "\n",
    "        Z_copy = Z.copy()\n",
    "        Z_copy = exp(Z_copy)\n",
    "        _sum = sum(Z_copy)\n",
    "        Z_copy = Z_copy / _sum\n",
    "        return Z\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "\n",
    "    def _one_hot(self, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        y = [2, 0, 4, 1, ..., N], N \\in \\mathbb{R}^{data points}\n",
    "        Y = np.zeros((y.shape[0], 5), dtype=float32)\n",
    "        for i in range(len(y)):\n",
    "            Y[i] = y[i]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _compute_loss(self, X:np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        cross-entropy loss\n",
    "        d = 2 features\n",
    "        K = 3 classes\n",
    "        batch_num = N (e.g. 4)\n",
    "\n",
    "        shapes:\n",
    "            X: N x d (4 x 2)\n",
    "            W: d x k (2 x 3)\n",
    "            b: 1 x k (bias)\n",
    "            y: 1 x N (example true classes)\n",
    "\n",
    "        compute logits:\n",
    "            X = \\begin{bmatrix}\n",
    "                    1, 1\n",
    "                    1, 2\n",
    "                    2, 1\n",
    "                    2, 2\n",
    "                \\end{bmatrix}\n",
    "\n",
    "            W = \\begin{bmatrix}\n",
    "                    0.1, -.2, -0.1\n",
    "                    0.0, 0.1, 0.2\n",
    "                \\end{bmatrix}\n",
    "\n",
    "            b = \\begin{bmatrix}\n",
    "                    0, 0, 0\n",
    "                \\end{bmatrix}\n",
    "\n",
    "            y = \\begin{bmatrix}\n",
    "                    1, 2, 0, 1\n",
    "                \\end{bmatrix}\n",
    "\n",
    "            Z = XW + b\n",
    "\n",
    "            z(0,0)=0.1\n",
    "            z(0,1)=0.3\n",
    "            z(0,2)=0.1\n",
    "\n",
    "            z(1,0)=0.1\n",
    "            z(1,1)=0.4\n",
    "            z(1,2)=0.3\n",
    "\n",
    "            ...\n",
    "\n",
    "            z(3,2)=0.2\n",
    "\n",
    "            Z = \\begin{bmatrix}\n",
    "                    0.1, 0.3, 0.1\n",
    "                    0.1, 0.4, 0.3\n",
    "                    0.2, 0.5, 0.0\n",
    "                    0.2, 0.6, 0.2\n",
    "                \\end{bmatrix}\n",
    "\n",
    "        compute softmax row-by-row:\n",
    "            P_{i,k} = \\frac{e^{Z_{i,k}}{\\sum_j e^{Z_{i,j}}\n",
    "\n",
    "            exp(0.1)=1.105\n",
    "            exp(0.3)=1.350\n",
    "            exp(0.1)=1.105\n",
    "\n",
    "            sum = 3.560\n",
    "\n",
    "            P(0,0)=0.31\n",
    "            P(0,1)=0.38\n",
    "            P(0,2)=0.31\n",
    "\n",
    "            P = \\begin{bmatrix}\n",
    "                    0.31, 0.38, 0.31\n",
    "                    0.28, 0.38, 0.34\n",
    "                    0.32, 0.43, 0.26\n",
    "                    0.29, 0.43, 0.29\n",
    "                \\end{bmatrix}\n",
    "\n",
    "            y = _one_hot(self, y) -> 4 x 3\n",
    "\n",
    "            Y = \\begin{bmatrix}\n",
    "            log(0.38)\n",
    "            log(0.34)\n",
    "            log(0.316)\n",
    "            log(0.427)\n",
    "                \\end{bmatrix}\n",
    "\n",
    "            loss = (all logs) / (N -> 4)\n",
    "\n",
    "        return loss\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def score(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
